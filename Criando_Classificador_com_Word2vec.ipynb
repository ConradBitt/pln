{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Criando Classificador com Word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXWIEguRlHE6v7A9bbux7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConradBitt/processamento_linguagem_natural/blob/master/Criando_Classificador_com_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ixRzUCfNNA0"
      },
      "source": [
        "# Introdução\n",
        "\n",
        "Este notebook é uma complementação do [Word2vec_Treinando_word_embedding](https://github.com/ConradBitt/processamento_linguagem_natural/blob/master/Word2vec_Treinando_word_embedding.ipynb) onde treinamos modelos. Aqui vamos utiliza-los para criar um classificador.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWMlqmqONqfy"
      },
      "source": [
        "# Importações de módulos e instalando idioma do spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnc6ffF2NzcM",
        "outputId": "9120fc9d-7c2c-4474-add2-a7f275bb8dcb"
      },
      "source": [
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import spacy\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "sns.set_context('talk')\n",
        "\n",
        "# Executar apenas uma vez e depois reiniciar o ambiente de execução.\n",
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "print('\\n\\n~ Versão python ~')\n",
        "!python --version\n",
        "\n",
        "print('\\n\\n~ Versões dos Módulos ~')\n",
        "print(f'Pandas: {pd.__version__}')\n",
        "print(f'Seaborn: {sns.__version__}')\n",
        "print(f'Numpy: {np.__version__}')\n",
        "print(f'Matplotlib: {mpl.__version__}')\n",
        "print(f'Spacy: {spacy.__version__}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pt_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz#egg=pt_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\n",
            "\n",
            "~ Versão python ~\n",
            "Python 3.7.10\n",
            "\n",
            "\n",
            "~ Versões dos Módulos ~\n",
            "Pandas: 1.1.5\n",
            "Seaborn: 0.11.1\n",
            "Numpy: 1.19.5\n",
            "Matplotlib: 3.2.2\n",
            "Spacy: 2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uht3BdpICi15"
      },
      "source": [
        "## Carregando idioma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMOBVuSSCfr_"
      },
      "source": [
        "# carregando o idioma\n",
        "nlp = spacy.load('pt_core_news_sm', disable=['parser','ner','tagger','textcat'])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVtUXN8RC1cC"
      },
      "source": [
        "## Montando Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO_H8HYqC2sh",
        "outputId": "8c9c4ec7-5f6d-4c16-f789-9f355a458553"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZppxCptCguc"
      },
      "source": [
        "## Carregando modelos já treinados com gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACfMgkGqCqBE"
      },
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVI2KfRRCoiY"
      },
      "source": [
        "cbow = '/content/drive/MyDrive/Colab Notebooks/word2vec/modelos/w2v_cbow_300.txt'\n",
        "skipgram = '/content/drive/MyDrive/Colab Notebooks/word2vec/modelos/w2v_skipgram_300.txt'\n",
        "\n",
        "w2v_cbow = KeyedVectors.load_word2vec_format(cbow)\n",
        "w2v_skipgram = KeyedVectors.load_word2vec_format(cbow)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzrxl6PKQDSv"
      },
      "source": [
        "# Pre processamento dos dados de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PI21AiRG5oN"
      },
      "source": [
        "## Importando dados de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiqkOQyKG8rt"
      },
      "source": [
        "treino = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/word2vec/dados/treino.csv')\n",
        "teste = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/word2vec/dados/teste.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C-LiAJ3QQKL"
      },
      "source": [
        "---\n",
        "## Tokenizador\n",
        "``tokenizador`` é uma função que recebe uma string qualquer e a partir dela cria um objeto do tipo ``Doc`` do SpaCy. Esta estrutura facilita análise de alguns textos, por exemplo, se é stopword ou não, se é alphanumerico ou não, etc. Por fim vamos adicionar o texto à uma lista de tokens validados pelo pre processamento e retorna-la."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVGqk9SaQoX1",
        "outputId": "ddb38139-1675-4951-fa4d-3cb89182536e"
      },
      "source": [
        "\n",
        "def tokenizador(texto):\n",
        "  \"\"\"\n",
        "  Esta função recebe um texto\n",
        "  cria um doc do spacy através do texto recebido\n",
        "  remove stopwords, retorna alphanumericos, deixa minusculo\n",
        "  tokeniza o texto e retorna uma lista de tokens\n",
        "  \"\"\"\n",
        "  tokens_validos = []\n",
        "  doc = nlp(texto)\n",
        "  for token in doc: \n",
        "    # variável de validação\n",
        "    # garante que o token não é stopword e é alphanumerico\n",
        "    e_valido = not token.is_stop and token.is_alpha\n",
        "    if e_valido:\n",
        "      tokens_validos.append(token.text.lower())\n",
        "      \n",
        "  return tokens_validos\n",
        "\n",
        "texto_de_teste = 'Rio de Janeiro 908175!@&$*!@éé uma cidade maravilhosa!'\n",
        "\n",
        "#testando tokenizador\n",
        "tokens_teste = tokenizador(texto_de_teste)\n",
        "print(tokens_teste)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rio', 'janeiro', 'cidade', 'maravilhosa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv2-ughnTzjn"
      },
      "source": [
        "note que foi retirado números, caracteres não alphabeticos e stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDK4hsiDR9Og"
      },
      "source": [
        "---\n",
        "## Combinação linear de tokens\n",
        "\n",
        "A ``combinacao_linear_tokens`` incorpora os tokens à um espaço vetorial. Basicamente se tem um espaço vetorial de ordem bem grande, 300, 500, 1000, ou mais. Cada token pode ser representado neste espaço vetorial e portanto o modelo cria uma combinação linear (soma) de vetores para expressar a frase. \n",
        "\n",
        "Como existe mais de uma arquitetura de modelo (CBOW e SG) será adicionado um parâmetro para que o modelo seja fornecido para produzir a combinação linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-NhvD2iQVCT"
      },
      "source": [
        "\n",
        "def combinacao_linear_tokens(tokens, modelo):\n",
        "  token_vetorizado = np.zeros(300)\n",
        "\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      # soma os vetores de cada token pra construir um vetor da frase.\n",
        "      token_vetorizado += modelo.get_vector(token)\n",
        "    except KeyError:\n",
        "      #print(f'\\033[31m KeyError: O token \"{token}\" não esta no vocabulario. \\033[0;0m')\n",
        "      pass\n",
        "\n",
        "  return token_vetorizado\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSeU_fR6DzZe"
      },
      "source": [
        "testando combinação linear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrudvVPwD2Dt",
        "outputId": "f4d17b8b-6139-4b09-c584-7c2b4cc878f8"
      },
      "source": [
        "token_teste_cbow = combinacao_linear_tokens(tokens_teste, w2v_cbow)\n",
        "token_teste_skipgram = combinacao_linear_tokens(tokens_teste, w2v_skipgram)\n",
        "\n",
        "print(f'CBOW {token_teste_cbow.shape}\\nSkipGram {token_teste_skipgram.shape}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW (300,)\n",
            "SkipGram (300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N76Le-UHEszA"
      },
      "source": [
        "conseguimos transformar em vetores a frase utilizando dois modelos: Skip Gram e Continous Bag of Words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm7pO3gQTsEq"
      },
      "source": [
        "---\n",
        "\n",
        "## Tensorizando frases\n",
        "\n",
        "Essa função vai ser responsável por pegar todas as frases e vetoriza-las, logo teremos uma matriz de 300 colunas e linhas iguais à quantidade de observações disponííveis, isto é, de titulos de notícias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XWGwoZMN0Ny"
      },
      "source": [
        "def tensoriza(array_texto, modelo):\n",
        "  \"\"\"\n",
        "  Variável array_texto é uma lista de palavras\n",
        "  onde cada palavra será tokenizada e em seguida\n",
        "  criada a sua representação vetorial e por fim,\n",
        "  armazenada numa matriz. Matriz que é retornada.\n",
        "  \"\"\"\n",
        "  x = len(array_texto) # quantidade de observações, titulos\n",
        "  y = 300  # colunas, ordem do espaço vetorial\n",
        "\n",
        "  matriz = np.zeros((x,y))\n",
        "\n",
        "  for indice in range(x):\n",
        "    palavras = tokenizador(array_texto.iloc[indice])\n",
        "    matriz[indice] = combinacao_linear_tokens(palavras, modelo)\n",
        "    \n",
        "  return matriz "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX93669lGve_"
      },
      "source": [
        "# Avaliando modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnCkO44dq702"
      },
      "source": [
        "## Vetorizando treino e teste com modelo CBOW\n",
        "\n",
        "Este processo pode demorar um pouco, pois o processo será realizado 90000 vezes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWELqKUBJvSb",
        "outputId": "dc324bc0-74ce-41b7-f1e1-20ce4044c821"
      },
      "source": [
        "treino.title.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6ldiNWGyu_",
        "outputId": "2ba4940b-9289-4cb2-eab6-efb2f0971fc5"
      },
      "source": [
        "%%time\n",
        "\n",
        "tensor_treino_cbow = tensoriza(treino.title, w2v_cbow)\n",
        "tensor_teste_cbow = tensoriza(teste.title, w2v_cbow)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 45min 17s, sys: 1min 33s, total: 1h 46min 51s\n",
            "Wall time: 1h 46min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1Eyo1pBHaTl",
        "outputId": "2de66938-a81c-4270-c4ed-09e247a5cd73"
      },
      "source": [
        "print(tensor_treino_cbow.shape)\n",
        "print(tensor_teste_cbow.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90000, 300)\n",
            "(20513, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFRMe0Gd8J9"
      },
      "source": [
        "### Instenciando modelos do Sci-kit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84nXblYjHds7"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edkHis46nD7K"
      },
      "source": [
        "Como temos dois tipos de arquitetura de word2vec, vou criar uma função que irá treinar e ajustar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L9onc87eDie"
      },
      "source": [
        "def classificador(modelo, x_treino, y_treino, x_teste, y_teste):\n",
        "  logistic_regression = LogisticRegression(max_iter = 350)\n",
        "  logistic_regression.fit(x_treino, y_treino)\n",
        "\n",
        "  categoria_predita = logistic_regression.predict(x_teste)\n",
        "  print(classification_report(y_teste, categoria_predita))\n",
        "\n",
        "  return logistic_regression"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbm-iWAn-d1"
      },
      "source": [
        "Usando a função"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxH6uNbYoBIA",
        "outputId": "723edd2c-ab42-4887-e789-c914ae3c2948"
      },
      "source": [
        "%%time\n",
        "\n",
        "lregression_cbow = classificador(w2v_cbow,\n",
        "                                 x_treino = tensor_treino_cbow,\n",
        "                                 y_treino = treino.category,\n",
        "                                 x_teste = tensor_teste_cbow,\n",
        "                                 y_teste = teste.category)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.81      0.71      0.76      6103\n",
            "   cotidiano       0.63      0.80      0.70      1698\n",
            "     esporte       0.92      0.86      0.89      4663\n",
            "   ilustrada       0.13      0.85      0.23       131\n",
            "     mercado       0.84      0.78      0.81      5867\n",
            "       mundo       0.74      0.83      0.78      2051\n",
            "\n",
            "    accuracy                           0.79     20513\n",
            "   macro avg       0.68      0.81      0.70     20513\n",
            "weighted avg       0.82      0.79      0.80     20513\n",
            "\n",
            "CPU times: user 1min 13s, sys: 14.4 s, total: 1min 27s\n",
            "Wall time: 45.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3zKzZ-SpTZY"
      },
      "source": [
        "#### Comentario sobre desempenho do CBOW \n",
        "\n",
        "> Este é o resultado do modelo, uma revocação média de 80% e uma acurácia média de 79% em prever os títulos. Entretanto, podemos verificar que a precisão na categoria ``ilustrada`` é de 13%, entretanto isso pode ser explicado pela pequena quantidade de amostras, apenas 131. **Por isso neste caso podemos avaliar a média ponderada (``weighted average``) que tem uma precisão ou valor preditivo positivo de 80% e uma revocação ou sensibilidade de 79%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkZ_lUoqpkje"
      },
      "source": [
        "## Vetorizando treino e teste com modelo Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae8i0H6irJeC",
        "outputId": "557f10f9-10be-4f15-da71-3a73f00ff84c"
      },
      "source": [
        "%%time\n",
        "\n",
        "tensor_treino_skipgram = tensoriza(treino.title, w2v_skipgram)\n",
        "tensor_teste_skipgram = tensoriza(teste.title, w2v_skipgram)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 54min 32s, sys: 1min 28s, total: 1h 56min\n",
            "Wall time: 1h 55min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIZhQx5HVQA"
      },
      "source": [
        "foi significativamente maior que o CBOW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZsX--jrQFi",
        "outputId": "248be172-0004-4b11-915f-2e5a3ec464c7"
      },
      "source": [
        "print(tensor_treino_skipgram.shape)\n",
        "print(tensor_teste_skipgram.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90000, 300)\n",
            "(20513, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r07C-nGsrWgf"
      },
      "source": [
        "### Usando classificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvZTAo80raH0",
        "outputId": "081bd54a-b716-4fc1-af6a-10ce53254cd1"
      },
      "source": [
        "%%time\n",
        "\n",
        "lregression_skipgram = classificador(w2v_skipgram,\n",
        "                                     x_treino = tensor_treino_skipgram,\n",
        "                                     y_treino = treino.category,\n",
        "                                     x_teste = tensor_teste_skipgram,\n",
        "                                     y_teste = teste.category)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.81      0.71      0.76      6103\n",
            "   cotidiano       0.63      0.80      0.70      1698\n",
            "     esporte       0.92      0.86      0.89      4663\n",
            "   ilustrada       0.13      0.85      0.23       131\n",
            "     mercado       0.84      0.78      0.81      5867\n",
            "       mundo       0.74      0.83      0.78      2051\n",
            "\n",
            "    accuracy                           0.79     20513\n",
            "   macro avg       0.68      0.81      0.70     20513\n",
            "weighted avg       0.82      0.79      0.80     20513\n",
            "\n",
            "CPU times: user 1min 13s, sys: 14.1 s, total: 1min 27s\n",
            "Wall time: 45.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGQH-MdOrxcK"
      },
      "source": [
        "#### Comentando sobre desempenho do skipgram\n",
        "\n",
        "> Dado que o desempenho das duas arquiteturas de Word2Vec foram similares, neste caso de classificação de títulos o SkipGram não é recomendado, pois o tempo de treino desses 90mil dados foi de 1h 56min contra a 1h 46min do CBOW.\n",
        "\n",
        "> Apesar do modelo SkipGram ter um desempenho melhor, um preditivo positivo da media ponderada de 82% e uma sensibilidade de 79%, isso vai depender muito do tipo de dado com que se esta trabalhando. \n",
        "\n",
        "> Na conclusão do notebook [Word2Vec: Interpretação da linguagem com word embedding](https://github.com/ConradBitt/processamento_linguagem_natural/blob/master/Word2Vec_Interpreta%C3%A7%C3%A3o_da_linguagem_com_word_embedding_.ipynb) discutimos sobre o tamanho dos dados com quais as arquiteturas lidam, isto é, testamos as arquiteturas prevendo titulos com frases até 10 palavras e com artigos inteiros. Verifiquei que: **Dada a natureza do SkipGram prever o contexto a traves de uma palavra quando a palavra pertence à um artigo inteiro é relativamente dificil pra esta arquitetura entregar bons resultados. Ja a caracteristica do CBOW de prever uma palavra dado um contexto de uma frase tem um desempenho muito melhor com dados muitos longos.**\n"
      ]
    }
  ]
}