{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural\n",
    "\n",
    "Vou abordar o conteito de processamento de linguagem natural ou [PLN](https://pt.wikipedia.org/wiki/Processamento_de_linguagem_natural). Esta é uma das técnicas que o Google utiliza para interpretar o que foi escrito na barra de busca dele. Geralmente outros buscadores fazem isto também, mas o google é o mais utilizado.\n",
    "\n",
    "Como ele faz para entender o que você esta querendo dizer?? Bom: Processamento de Linguagem Natural.\n",
    "Com a PLN o google pode trazer os resultados mais relevantes para sua pesquisa, assim como o Google Home, Alexa ou Siri fazem para entender o que você esta falando... Tudo isso é processamento de linguagem natural.\n",
    "\n",
    "Vamos iniciar desenvolvendo um analisador de sentimentos através da NPL(Natural Processing Language), mas como isso funciona??\n",
    "\n",
    "Imagina que você quer assistir um filme então você vai lá e busca criticas sobre o filme por exemplo no site [Adoro Cinema](http://www.adorocinema.com/). Esta critica do filme você vai interpretar e aí você irá classificar como bom ou ruim, ou com estrelas... **E é exatamente isto que nosso algorítmo irá fazer: Classificar críticas de filmes**.\n",
    "\n",
    "Para isso vamos:\n",
    "\n",
    "* Importar uma base de dados já classificada.\n",
    "* Criar um modelo de machine learn capaz de interpretar a lingaugem humana e classificar este texto (atráves de *WORD CLOUDS*).\n",
    "* Pré processamento de texto para torna-lo mais preciso.\n",
    "* Usar a distribuição de pareto para ajudar na análise de dados.\n",
    "* Realizar Tokenização com o [NLTK](https://www.nltk.org/)\n",
    "\n",
    "## Começando\n",
    "\n",
    "Vamos usar uma base de dados do IMDB disponível do [kaggle](https://www.kaggle.com/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#para que os algorítmos peguem sempre a mesma semente\n",
    "import numpy as np \n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resenha = pd.read_csv('./dados/imdb-reviews-pt-br.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49454</th>\n",
       "      <td>49456</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>Como a média de votos era muito baixa, e o fat...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49455</th>\n",
       "      <td>49457</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>O enredo teve algumas reviravoltas infelizes e...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49456</th>\n",
       "      <td>49458</td>\n",
       "      <td>I am amazed at how this movieand most others h...</td>\n",
       "      <td>Estou espantado com a forma como este filme e ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49457</th>\n",
       "      <td>49459</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>A Christmas Together realmente veio antes do m...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49458</th>\n",
       "      <td>49460</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>O drama romântico da classe trabalhadora do di...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49459 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "49454  49456  Seeing as the vote average was pretty low, and...   \n",
       "49455  49457  The plot had some wretched, unbelievable twist...   \n",
       "49456  49458  I am amazed at how this movieand most others h...   \n",
       "49457  49459  A Christmas Together actually came before my t...   \n",
       "49458  49460  Working-class romantic drama from director Mar...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg  \n",
       "...                                                  ...       ...  \n",
       "49454  Como a média de votos era muito baixa, e o fat...       pos  \n",
       "49455  O enredo teve algumas reviravoltas infelizes e...       pos  \n",
       "49456  Estou espantado com a forma como este filme e ...       pos  \n",
       "49457  A Christmas Together realmente veio antes do m...       pos  \n",
       "49458  O drama romântico da classe trabalhadora do di...       pos  \n",
       "\n",
       "[49459 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resenha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o sklearn\n",
    "\n",
    "Dentro da biblioteca sci-kit learn no módulo **model_selection** tem uma função chamada `train_test_split` que faz justamente o que precisamos: \n",
    "\n",
    "* Reservar dados para treinar o modelo.\n",
    "* Reservar dados para testar o modelo \n",
    "\n",
    "Esta função esta disponível aqui: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Como argumento a função `train_test_split` recebe: `data_x`, `data_y`, `test_size`\n",
    "\n",
    "O retorno dela é: `x_treino`, `x_teste`, `y_treino`,`y_teste`\n",
    "\n",
    "Em alguns lugares a coluna $y$ é chamada de classificador porque entendemos o modelo como uma função $f()$. Desta forma temos $x_{treino}$ que sera aplicado no modelo $f()$ e irá gerar o $y_{treino}$ tal que: \n",
    "\n",
    "$y_{treino} = f(x_{treino})$\n",
    "\n",
    "Com o $y_{treino}$ comparamos com o $y_{teste}$. Ou seja, $y$ é a classificação do conjunto de dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino, x_teste, classe_treino, classe_teste = train_test_split(resenha.text_pt,\n",
    "                                                                  resenha.sentiment,\n",
    "                                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando o SKlearn Linear Model Logistic Regression \n",
    "\n",
    "O modelo de regressão linear usando a função logistica. A função [logistica](https://en.wikipedia.org/wiki/Logistic_function) ou também conhecida como função logística, até sigmoide, é uma curva em forma de \"S\"\n",
    "\n",
    "<img src='./images/Logistic-curve.png' >\n",
    "\n",
    "É sabido na literatura que a curva logistica é usada como classificador de uma logistic regression, maximum-entropy classification e linear-log classification.\n",
    "\n",
    "Este modelo esta disponível em: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Embora o filme tenha sido apenas assim, o closed caption foi de longe o melhor que eu já vi! Na maioria das vezes, a ortografia é terrível e a legenda está fora de sincronia. Eu uso o closed captioning mesmo que eu possa ouvir bem, mas acho que muitos atores resmungam. Também muitas vezes a trilha sonora substitui o diálogo. Obrigado!'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00bf431a85a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mregressao_logistica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#treinando modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mregressao_logistica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasse_treino\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#testando o modelo: acuracia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0macuracia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressao_logistica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_teste\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasse_teste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    752\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Embora o filme tenha sido apenas assim, o closed caption foi de longe o melhor que eu já vi! Na maioria das vezes, a ortografia é terrível e a legenda está fora de sincronia. Eu uso o closed captioning mesmo que eu possa ouvir bem, mas acho que muitos atores resmungam. Também muitas vezes a trilha sonora substitui o diálogo. Obrigado!'"
     ]
    }
   ],
   "source": [
    "#instanciando modelo\n",
    "regressao_logistica = LogisticRegression()\n",
    "#treinando modelo\n",
    "regressao_logistica.fit(x_treino, classe_treino)\n",
    "#testando o modelo: acuracia \n",
    "acuracia = regressao_logistica.score(x_teste, classe_teste)\n",
    "#exibindo acuracia\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE O ERRO: \n",
    "\n",
    "O erro acima indica que o modelo não consegue converter uma `string` em `float`. É neste ponto em que entra o NLP entra, pra fazer a conversão entre linguagem natural para uma linguagem de máquina.\n",
    "\n",
    "## Mas antes:\n",
    "\n",
    "Temos que salientar alguns pontos de NLP. \n",
    "\n",
    "Primeiro, precisamos conhecer nossa base de dados, ou seja os textos. Se são longos ou curtos, como ela se comporta, se temos dicas que podem existir no texto para os classificadores para ajudar a definir se é positivo ou negativo. \n",
    "\n",
    "Por exemplo: `Filme ok` essa resenha é dificil de classificar, mas esta `Filme ok :(` é mais fácil, pois temos uma carinha triste `:(`. Outro exemplo seria `Filme ok :)`\n",
    "\n",
    "Bom mas isso aqui não é o que aparece na nossa base de dados, nós temos textos longos, que precisam ser submetidos à uma análise mais fina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Negativa\\n')\n",
    "\n",
    "print(resenha.text_pt[189])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positivo\\n')\n",
    "\n",
    "print(resenha.text_pt[49002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante ler um pouco sobre a base de dados, pois prá nós é simples ler um texto e extrair a opinião superficial da pessoa que o escreveu, mas isso não é tão fácil para o computador. Por isso ao ler podemos identificar algumas características de filmes classificados com positivos e negativos, por exemplo a palavra `'pior'` ou `'BRILHANTE!!!'`\n",
    "\n",
    "# outro ponto importante:\n",
    "\n",
    "A quantidade de dados. Não é novidade que para aprendizado de máquina seja necessário muitos dados, nossa base por exemplo tem 50K resenhas, mas pra que nosso modelo não seja enviesado é DE SUMA IMPORTÂNCIA que os dados sejam equivalentes, ou seja, a mesma quantidade de negativos tenha de positivos ou que pelo você faça um pré processamento para que ele fiquem balanceados. Isso é importante porque evita que o modelo fique enviesado e comece a achar que a maioria das resenhas são positivas ou negativas... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quantidade de avaliações positivas e negativas:\n",
    "resenha.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resenha.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aletarndo para classificação binaria\n",
    "\n",
    "Note que temos avialiação de sentimentos como `neg` e `pos`. Então vamos vazer um mapeamento de `neg → 0` e `pos → 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao = resenha['sentiment'].replace(['neg','pos'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resenha['classificacao'] = classificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resenha.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora já transformamos um dado textual em numérico sem perder o sentido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Lingugagem Natural\n",
    "\n",
    "É uma subárea da inteligência artificial que se preocupa em estudar a estrutura que gerencia a comunicação entre linguagem natural (humanos, português, inglês...) e a linguagem de máquina (computador, bits).\n",
    "\n",
    "Dentro da PLN existe a analise textual, análise de sentimentos por texto, por imagens, enfim, várias outras subáreas...\n",
    "\n",
    "## Vamos transformar texto em linguagem de máquina:\n",
    "\n",
    "Quando temos as frases, ou melhor, na linguagem de PLN, o *corpus*:\n",
    "\n",
    "    O filme é muito bom.\n",
    "    O filme é muito ruim.\n",
    "    \n",
    "No nosso caso, todas as resenhas compõe nosso *corpus textual*. Neste exemplo acima, nosso corpus textual é bem mais simples. Similar ao meu post [\"Isso é ou não um porquinho\"](https://medium.com/@opolidordelentes/isso-%C3%A9-porquinho-ou-cachorrinho-f65bfd0215b4), podemos estratificar essa frase e atribuir uma classificação binária a cada palavra... \n",
    "\n",
    "Vamos criar um **Vocabulário**. Este vocabulário vai ter todas as minhas palavras mas sem repetições.\n",
    "\n",
    "Aplicando a função `split()` nas frases por exemplo, temos:\n",
    "```\n",
    "['O', 'filme', 'é', 'muito', 'bom']\n",
    "['O', 'filme', 'é', 'muito', 'ruim']\n",
    "```\n",
    "\n",
    "adicionando os elementos dessa lista em um `set()` temos:\n",
    "`['O', 'filme', 'é', 'muito', 'bom', 'ruim']`\n",
    "Então montamos uma matriz, com as letras sendo as colunas: \n",
    "\n",
    "<img src='./images/vocabulario.png' width=50%>\n",
    "\n",
    "Ou seja, cada frase pode ser representada por um **vetor** e de forma bem simples podemos classificar uma frase como **boa** ou **ruim**. Apesar de ser uma forma simples é muito poderosa, então use abuse deste método de usando nas colunas variáveis linearmente independentes.\n",
    "\n",
    "Mas e a frase: \n",
    "    \n",
    "    O filme é muito muito bom.\n",
    "    \n",
    "como podemos enfatisar uma palavra?? Bom podemos fazer da seguinte maneira: \n",
    "\n",
    "<img src='./images/vocabulario_com_palavra_dupla.png' width=60%>\n",
    "\n",
    "Essa forma vetorial de tratar frases pode ser uma forma bem eficiente além de modelar uma frase, consegue inferir uma intensidade a cada palavra! Para saber mais sobre esta representação procure por **\"Sacola de palavras\" ou [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)**.\n",
    "\n",
    "\n",
    "# Vamos então criar nosso Bag of Words:\n",
    "\n",
    "Nosso dataset do IMDB possui quase 50 mil registros. Esse é um conjunto de dados grande e pidemos utilizar um dataframe do pandas parada guardar as informações dele.\n",
    "\n",
    "Como acabamos de ver uma bag of words cria um label para cada palavra e caso isso se aplique no nosso dataset é provavel que tenhamos mais elementos *nulos* do que elementos preenchidos, quando isso acontece chamamos essa matriz de **matriz esparsa** ou **[sparce matrix](https://en.wikipedia.org/wiki/Sparse_matrix)**\n",
    "\n",
    "<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/0fce3708488472b34a85f11f54d8df3eeab1aabc'>\n",
    "\n",
    "Não é muito eficiente armazenar toda a matriz, porque alocar valores 0 (zero é diferente de null) pode ser um desperdicio de memória e em alguns casos levar ao **[overflow](https://en.wikipedia.org/wiki/Buffer_overflow)**, por isso cuidado ao trabalhar com matrizes esparsas. Para isso o pandas tem uma estrutura especial para trabalhar com matrizes esparsas, essa estrutura armazena apenas os valores diferentes de zero em uma entidade de controle.\n",
    "\n",
    "Nós vamos usar o `DataFrame.sparse.from_spmatrix` do pandas, para usar o `DataFrame.sparse.from_matrix` passamos como parâmetro:\n",
    "\n",
    "```\n",
    "pandas.DataFrame.sparse.from_matrix(bag_of_words, columns=vetorizar.get_feature_names())\n",
    "```\n",
    "Então o `DataFrame.sparse.from_matrix` recebe uma matriz esparsa (variável matriz_esparsa) e nos retorna um Dataframe (variável do tipo dataframe). Para mais informações busque em: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sparse.from_spmatrix.html\n",
    "\n",
    "A principio vamos utilizar o objeto `CountVectorizer()` que cria um modelo que irá vetorizar as palavras que são passadas à ele pela fução `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = ['Assisti um filme ótimo', 'Assisti um filme ruim']\n",
    "\n",
    "# vetorizar porque esse objeto é responsável por vetorizar frases\n",
    "vetorizar_frase = CountVectorizer(lowercase=False)\n",
    "\n",
    "#contruindo sacola de palavras\n",
    "bag_of_words = vetorizar_frase.fit_transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorizar_frase.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_sparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words,\n",
    "                                                  columns=vetorizar_frase.get_feature_names())\n",
    "matriz_sparsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é importante usar o objeto `DataFrame.sparse.from_spmatrix` para construir a matriz sparsa \n",
    "\n",
    "\n",
    "# Aplicando em nosso database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetorizar porque esse objeto é responsável por vetorizar frases\n",
    "vetorizar_frase = CountVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "#contruindo sacola de palavras\n",
    "bag_of_words = vetorizar_frase.fit_transform(resenha.text_pt)\n",
    "\n",
    "print(bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que este vetor de dimensão $(49459, 156923)$ é caro computacionalmente falando... Um jeito de contornar esse problema **com perda de informação** é ao instanciar o `CountVectorizer()` passar o parâmetro `max_features` isso vai limitar a ordem do vetor que será retornado e ele só irá retornar as palavras que mais aparecem...\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados em treino e teste\n",
    "treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words, \n",
    "                                                             resenha.classificacao,\n",
    "                                                             random_state=42)\n",
    "# ajustando o modelo \n",
    "regressao_logistica = LogisticRegression()\n",
    "regressao_logistica.fit(treino, classe_treino)\n",
    "acuracia = regressao_logistica.score(teste, classe_teste)\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar_texto(texto, coluna_texto, coluna_classificacao):\n",
    "    \n",
    "    vetorizar_frase = CountVectorizer(lowercase=False, max_features=50)\n",
    "    bag_of_words = vetorizar_frase.fit_transform(texto[coluna_texto])\n",
    "    \n",
    "    #separando em dados de treino e testo:\n",
    "    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words, \n",
    "                                                                 texto[coluna_classificacao],\n",
    "                                                                 random_state=42)\n",
    "    # ajustando o modelo \n",
    "    regressao_logistica = LogisticRegression()\n",
    "    regressao_logistica.fit(treino, classe_treino)\n",
    "    return regressao_logistica.score(teste, classe_teste)\n",
    "\n",
    "\n",
    "print(classificar_texto(resenha,\"text_pt\", \"classificacao\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como agora já temos a precisão do modelo pra determinar qual a taxa de acerto de um dado filme, vamos colocar as palavras com maior representatividade semântica na classificação de um filme sendo \"bom\" ou \"ruim\" estejam mais proximas o possível das *50 features* selecionadas pelo modelo através do parâmetro `max_features`. Para ter uma ideia melhor de quais são as 50 features mais \"relevantes\" do nosso corpus textual vamos usar uma técnica chamada **word_cloud**.\n",
    "\n",
    "# Word Cloud\n",
    "\n",
    "Você provavelmente já viu uma **word cloud** em algum site, buzzfeed por exemplo. Se não viu recomendo que use mais a internet (kkkkk) mas se você não reconhece pelo nome, talvez reconheça pela imagem:\n",
    "\n",
    "<img src='./images/word-cloud.png'>\n",
    "\n",
    "nesta representação visual de uma \"núvem de palavras\" o tamanho da palavra é proporcional à sua relevância assim fica relativamente fácil identificar quais palavras são mais importantes num database.\n",
    "\n",
    "Existe um repositório do github com um nome que não é tão intuitivo para gerar wordcloud, o repositório é este aqui: [repositório](https://github.com/amueller/word_cloud)\n",
    "\n",
    "Já a documentação desta biblioteca esta disponível [aqui](http://amueller.github.io/word_cloud/) \n",
    "\n",
    "A função que vamos utilizar se chama `wordcloud()` a descrição desta função esta aqui: \n",
    "http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud\n",
    "\n",
    "Ela recebe como parâmetro uma lista contendo todas as palavras a serem utilizadas pra construção da wordcloud:\n",
    "\n",
    "Eu tentei rodar com todas as palavras na séries `resenha.text_pt` entretanto são, ao realizar o algorítmo \n",
    "\n",
    "```\n",
    "todas_as_palavras = ''.join([texto for texto in resenha.text_pt])\n",
    "```\n",
    "\n",
    "\n",
    "são retornadas 63 milhões palavras, o esforço computacional pra isso é muito alto. Portanto talvez seja necessário fazer apenas com 60% das frases contidas em `resenha.text_pt` que são 37 milhões de palavras. Selecionando amostras aleatórias com o `sample()` ou com uma % menor do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#60% das resenhas.text_pt\n",
    "#amostra_da_resenha = int(len(resenha.text_pt)*0.6)\n",
    "# atraǘes desta list comprehencion vamos contruir uma lista contendo todas as resenhas.\n",
    "#mas queremos UMA LISTA COM TODAS AS PALAVRAS, não com as frases. Então usamos o ' '.join(lista)\n",
    "todas_as_palavras = ' '.join([texto for texto in resenha.text_pt])\n",
    "\n",
    "\n",
    "\n",
    "# Aqui vem uma discussão importante: \n",
    "nuvem_de_palavras = WordCloud(width=800, # alterar width e height não alera o tamanho\n",
    "                              height=500, # da imagem apenas a área distribuição das palavras\n",
    "                              max_font_size=110, # altera o tamanho de cada fonte na wordcloud\n",
    "                              collocations=False).generate(todas_as_palavras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no que é um objeto\n",
    "nuvem_de_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar esse objeto com o matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(nuvem_de_palavras, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando a Word Cloud \n",
    "\n",
    "Podemos notar que as palavras \"não\",\"para\",\"que\",\"filme\",\"de\" são palavras que aparecem em maior destaque na wordcloud, mas séra que elas realmente ajudam a classificar se um filme é bom ou ruim?? Provávelmente não, e é exatamente esta função da WORD CLOUD! Nos ajudar a verificar se as palavras que estão sendo escolhidas como \"mais relevantes\" são realmente suficientes pra determinar se um filme é bom ou ruim, e melhorar os resultados. \n",
    "\n",
    "### Outro ponto\n",
    "\n",
    "Faz sentido olhar para as palavras \"positivas\" e \"negativas\" ao mesmo tempo?? Até certo ponto pode fazer sentido, mas talvez seja melhor olhar para as plavras \"positivas\" separada das \"negativas\", assim é possível fazer uma analise mais fina de como o algorítmo esta analisando as palavras pra classificar um comentário... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
